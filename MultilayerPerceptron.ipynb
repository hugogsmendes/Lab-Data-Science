{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiiDrCyFG/3whIbnGxALak",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hugogsmendes/Lab-Data-Science/blob/main/MultilayerPerceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w9E1e8er4-Ud"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de Ativação Sigmóide\n",
        "def sigmoid (z):\n",
        "  return 1.0/(1.0+np.exp(-z))"
      ],
      "metadata": {
        "id": "aqmrfeNHATdk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network (object):\n",
        "\n",
        "  def __init__(self, sizes):\n",
        "    self.num_layers = len(sizes) # Número de camadas\n",
        "    self.sizes = sizes # Número de neurônios em cada camada\n",
        "    self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "    self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "  def feedforward(self, a):\n",
        "    for b, w in zip (self.biases, self.weights):\n",
        "      a = sigmoid(np.dot(w, a) + b)\n",
        "    return a\n",
        "\n",
        "  def SGD (self, training_data: list[tuple], epochs, mini_batch_size, eta, test_data=None):\n",
        "\n",
        "    training_data = list(training_data)\n",
        "    n = len(training_data)\n",
        "\n",
        "    if test_data:\n",
        "      test_data = list(test_data)\n",
        "      n_test = len(test_data)\n",
        "\n",
        "    for j in range(epochs):\n",
        "      random.shuffle(training_data)\n",
        "      mini_batches = [\n",
        "          training_data[k:k+mini_batch_size]\n",
        "          for k in range(0, n, mini_batch_size)]\n",
        "      for mini_batch in mini_batches:\n",
        "        self.update_mini_batch(mini_batch, eta)\n",
        "      if test_data:\n",
        "        print(f\"Epoch {j} : {self.evaluate(test_data)} / {n_test}\")\n",
        "      else:\n",
        "        print(f\"Epoch {j} complete\")\n",
        "\n",
        "  def update_mini_batch(self, mini_batch, eta):\n",
        "\n",
        "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "    for x, y in mini_batch:\n",
        "      delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "      nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "      nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "\n",
        "    self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
        "    self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "  def backprop(self, x, y):\n",
        "\n",
        "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "    activation = x\n",
        "    activations = [x]\n",
        "    zs = []\n",
        "\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "      z = np.dot(w, activation) + b\n",
        "      zs.append(z)\n",
        "      activation = sigmoid(z)\n",
        "      activations.append(activation)\n",
        "\n",
        "    # Backward pass\n",
        "    delta = self.cost_derivative(activations[-1], y) * sigmoid(zs[-1])\n",
        "    nabla_b[-1] = delta\n",
        "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "    for l in range(2, self.num_layers):\n",
        "      z = zs[-l]\n",
        "      sp = sigmoid(z)*(1-sigmoid(z))\n",
        "      delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "      nabla_b[-l] = delta\n",
        "      nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "\n",
        "    return (nabla_b, nabla_w)"
      ],
      "metadata": {
        "id": "-I3up28h5alS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fFeJgd8iK4Nz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}